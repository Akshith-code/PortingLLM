# PortingLLM
Developed a fully offline, on-device Large Language Model (LLM) inference system for Android . The project focuses on efficient mobile inference, low memory usage, and reliable structured output generation.  Run LLMs locally on Android without internet or cloud APIs  Optimize performance, memory, and latency for mobile hardware
